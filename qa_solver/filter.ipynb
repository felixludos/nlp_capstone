{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/anaconda3/envs/py36/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from gensim.models.wrappers import FastText\n",
    "import gensim\n",
    "import fastText\n",
    "torch.set_printoptions(linewidth=120)\n",
    "np.set_printoptions(linewidth=120, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['elements', 'vecs', 'rows'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = torch.load('../fast_table.pth.tar')\n",
    "table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = dict(zip(table['elements'], table['vecs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([282594, 900])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "for row in table['rows']:\n",
    "    X.append(np.concatenate([word2vec[x] for x in row]))\n",
    "X = torch.from_numpy(np.stack(X)).float()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oie = torch.load('../oie_table.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = dict(zip(oie['elements'], oie['vecs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3793, 900])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for row in oie['rows']:\n",
    "    Y.append(np.concatenate([word2vec[x] for x in row]))\n",
    "Y = torch.from_numpy(np.stack(Y)).float()\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosd(x,y):\n",
    "    if x.ndimension() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    if y.ndimension() == 1:\n",
    "        y = y.unsqueeze(0)\n",
    "    x = F.normalize(x, 2, -1)\n",
    "    y = F.normalize(y, 2, -1)\n",
    "    return -x @ y.transpose(-1,-2)/2+.5\n",
    "def l2(x,y):\n",
    "    if x.ndimension() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    if y.ndimension() == 1:\n",
    "        y = y.unsqueeze(0)\n",
    "    x = x.unsqueeze(-2)\n",
    "    y = y.unsqueeze(-3)\n",
    "    return (x-y).pow(2).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2825 3.5261\n",
      "11/2825 3.2596\n",
      "21/2825 3.1955\n",
      "31/2825 3.2305\n",
      "41/2825 3.1767\n",
      "51/2825 3.2734\n",
      "61/2825 3.1280\n",
      "71/2825 3.4003\n",
      "81/2825 3.3374\n",
      "91/2825 2.7001\n",
      "101/2825 3.6292\n",
      "111/2825 3.3079\n",
      "121/2825 3.6398\n",
      "131/2825 4.1380\n",
      "141/2825 4.4164\n",
      "151/2825 4.0529\n",
      "161/2825 4.1734\n",
      "171/2825 4.1854\n",
      "181/2825 2.9403\n",
      "191/2825 4.0825\n",
      "201/2825 3.4101\n",
      "211/2825 3.9970\n",
      "221/2825 3.6527\n",
      "231/2825 3.5535\n",
      "241/2825 3.4749\n",
      "251/2825 3.1900\n",
      "261/2825 3.8819\n",
      "271/2825 3.8561\n",
      "281/2825 3.7243\n",
      "291/2825 3.7539\n",
      "301/2825 3.6800\n",
      "311/2825 3.5386\n",
      "321/2825 3.4891\n",
      "331/2825 4.0070\n",
      "341/2825 4.0178\n",
      "351/2825 3.6068\n",
      "361/2825 2.4361\n",
      "371/2825 2.2957\n",
      "381/2825 2.1924\n",
      "391/2825 2.2336\n",
      "401/2825 2.3418\n",
      "411/2825 2.3611\n",
      "421/2825 2.3720\n",
      "431/2825 2.3098\n",
      "441/2825 2.3404\n",
      "451/2825 2.3837\n",
      "461/2825 2.2881\n",
      "471/2825 2.4684\n",
      "481/2825 2.4510\n",
      "491/2825 2.5253\n",
      "501/2825 2.4827\n",
      "511/2825 2.6526\n",
      "521/2825 2.5579\n",
      "531/2825 2.3617\n",
      "541/2825 2.3797\n",
      "551/2825 2.2987\n",
      "561/2825 2.4152\n",
      "571/2825 2.6080\n",
      "581/2825 2.4728\n",
      "591/2825 2.4722\n",
      "601/2825 2.4842\n",
      "611/2825 2.3149\n",
      "621/2825 2.3765\n",
      "631/2825 2.5265\n",
      "641/2825 2.4546\n",
      "651/2825 2.4377\n",
      "661/2825 2.2279\n",
      "671/2825 2.3335\n",
      "681/2825 2.2475\n",
      "691/2825 2.2440\n",
      "701/2825 2.2826\n",
      "711/2825 2.2786\n",
      "721/2825 2.4461\n",
      "731/2825 2.4335\n",
      "741/2825 2.3772\n",
      "751/2825 2.2752\n",
      "761/2825 2.3750\n",
      "771/2825 2.5131\n",
      "781/2825 2.7543\n",
      "791/2825 2.3958\n",
      "801/2825 2.2525\n",
      "811/2825 2.4878\n",
      "821/2825 2.2849\n",
      "831/2825 2.3244\n",
      "841/2825 2.4596\n",
      "851/2825 2.4342\n",
      "861/2825 2.4358\n",
      "871/2825 2.4407\n",
      "881/2825 2.3994\n",
      "891/2825 2.5192\n",
      "901/2825 2.5058\n",
      "911/2825 2.2821\n",
      "921/2825 2.5115\n",
      "931/2825 2.6744\n",
      "941/2825 2.5375\n",
      "951/2825 2.3323\n",
      "961/2825 2.3464\n",
      "971/2825 2.4368\n",
      "981/2825 2.5383\n",
      "991/2825 2.4528\n",
      "1001/2825 2.4675\n",
      "1011/2825 2.4059\n",
      "1021/2825 2.4082\n",
      "1031/2825 2.6088\n",
      "1041/2825 2.2518\n",
      "1051/2825 2.3300\n",
      "1061/2825 2.3547\n",
      "1071/2825 2.4500\n",
      "1081/2825 2.2723\n",
      "1091/2825 2.3408\n",
      "1101/2825 2.2808\n",
      "1111/2825 2.3727\n",
      "1121/2825 2.6107\n",
      "1131/2825 2.7697\n",
      "1141/2825 2.7024\n",
      "1151/2825 2.5961\n",
      "1161/2825 2.8524\n",
      "1171/2825 2.4645\n",
      "1181/2825 2.6032\n",
      "1191/2825 2.4902\n",
      "1201/2825 2.5354\n",
      "1211/2825 2.5133\n",
      "1221/2825 2.3994\n",
      "1231/2825 2.4285\n",
      "1241/2825 2.5106\n",
      "1251/2825 2.3259\n",
      "1261/2825 2.2910\n",
      "1271/2825 2.2832\n",
      "1281/2825 2.2604\n",
      "1291/2825 2.4391\n",
      "1301/2825 2.2958\n",
      "1311/2825 2.5408\n",
      "1321/2825 2.4539\n",
      "1331/2825 2.5085\n",
      "1341/2825 2.3031\n",
      "1351/2825 2.4132\n",
      "1361/2825 2.4060\n",
      "1371/2825 2.4728\n",
      "1381/2825 2.2276\n",
      "1391/2825 2.5348\n",
      "1401/2825 2.4534\n",
      "1411/2825 2.4213\n",
      "1421/2825 2.4022\n",
      "1431/2825 2.2799\n",
      "1441/2825 2.3988\n",
      "1451/2825 2.4261\n",
      "1461/2825 2.1035\n",
      "1471/2825 2.6275\n",
      "1481/2825 2.3932\n",
      "1491/2825 2.5532\n",
      "1501/2825 2.2744\n",
      "1511/2825 2.3084\n",
      "1521/2825 2.6312\n",
      "1531/2825 2.6466\n",
      "1541/2825 2.5302\n",
      "1551/2825 2.5192\n",
      "1561/2825 2.5652\n",
      "1571/2825 2.7905\n",
      "1581/2825 2.5547\n",
      "1591/2825 2.7060\n",
      "1601/2825 2.6883\n",
      "1611/2825 2.6013\n",
      "1621/2825 2.5239\n",
      "1631/2825 2.3154\n",
      "1641/2825 2.4191\n",
      "1651/2825 2.3025\n",
      "1661/2825 2.4068\n",
      "1671/2825 2.5941\n",
      "1681/2825 2.4949\n",
      "1691/2825 2.4554\n",
      "1701/2825 2.3978\n",
      "1711/2825 2.5499\n",
      "1721/2825 2.4737\n",
      "1731/2825 2.6830\n",
      "1741/2825 2.6020\n",
      "1751/2825 2.8284\n",
      "1761/2825 2.3837\n",
      "1771/2825 2.2826\n",
      "1781/2825 2.5472\n",
      "1791/2825 4.4395\n",
      "1801/2825 2.2220\n"
     ]
    }
   ],
   "source": [
    "step = 100\n",
    "N = len(X)//step\n",
    "D = []\n",
    "dist_fn = l2\n",
    "for i in range(N):\n",
    "    start = time.time()\n",
    "    D.append(dist_fn(X[i*step:(i+1)*step], Y))\n",
    "    if i % 10 ==0:\n",
    "        print('{}/{} {:.4f}'.format(i+1,N, time.time()-start))\n",
    "    \n",
    "#D = torch.cat(D)\n",
    "#D.shape\n",
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
